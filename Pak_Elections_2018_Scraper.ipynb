{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "from time import sleep\n",
    "from random import randint, randrange\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "header = {'User-Agent':str(ua.firefox)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the dashboard website, get the URLs for each NA constituency result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get('https://www.ecp.gov.pk/resultdashboard/ge2018.aspx',headers=header)\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "# From the dashboard page, fetch URLs for all the NA seats\n",
    "na_urls = [i.find('a')['href'][2:].replace(' ','%20')\\\n",
    "           for i in soup.find_all(name='div',attrs={'class':'card card-hover'})\\\n",
    "           if i.find('a') != None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the links for all the available constituencies have been retrieved, scrape the voting stats and data from each and store them as a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = []\n",
    "count = 0\n",
    "\n",
    "for na_url in na_urls:\n",
    "    url = 'https://www.ecp.gov.pk' + na_url\n",
    "    seat = na_url[na_url.find('=') + 1 : na_url.find('&')]\n",
    "    print('\\n--- Fetching URL for Seat # : ' + str(seat))\n",
    "    print('--- URL : ' + url)\n",
    "    r = requests.get(url,headers=header)\n",
    "    if r.status_code == 200:\n",
    "        print('--- HTML retrieved. Extracting Data')\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    else:\n",
    "        print('*** Abort. HTML Status Code ' + str(r.status_code))\n",
    "        break\n",
    "    \n",
    "    # Extract seat info and result status\n",
    "    seatInfo = soup.find_all('span',{'id' : 'ContentPlaceHolder2_lblSubHeading'})[0].getText()\n",
    "    seatName = seatInfo[seatInfo.find('(') + 1 : seatInfo.find(')')]\n",
    "    seatStatus = seatInfo[seatInfo.find(')') + 1 : ].strip()\n",
    "    \n",
    "    # Extract voting statistics\n",
    "    stats = soup.find_all('table')[0]\n",
    "    registeredVoters = stats.find('span',{'id': 'ContentPlaceHolder1_lblRegVoters'}).getText()\n",
    "    votesPolled = stats.find('span',{'id': 'ContentPlaceHolder1_lblVotesPolled'}).getText()\n",
    "    validVotes = stats.find('span',{'id': 'ContentPlaceHolder1_lblValidVotes'}).getText()\n",
    "    rejectedVotes = stats.find('span',{'id': 'ContentPlaceHolder1_lblRejVotes'}).getText()\n",
    "    polledToRegisteredRatio = stats.find('span',{'id': 'ContentPlaceHolder1_lblTO'}).getText().replace('%','').strip()\n",
    "    \n",
    "    # Extract voting results\n",
    "    voteCount = soup.find_all('table')[1]\n",
    "    votingResults = []\n",
    "    for i in voteCount.find_all('tr'):\n",
    "        row = i.find_all('p')\n",
    "        if len(row) != 0:\n",
    "            candidateName = row[0].getText()\n",
    "            candidateParty = row[1].getText()\n",
    "            candidateVotes = row[2].getText()\n",
    "            votingDict = {'candidateName' : candidateName,\\\n",
    "                          'candidateParty' : candidateParty,\\\n",
    "                          'candidateVotes' : int(candidateVotes)}\n",
    "            votingResults.append(votingDict)\n",
    "        \n",
    "    data = {'seat' : seat,\\\n",
    "            'seatName' : seatName,\\\n",
    "            'seatStatus' : seatStatus,\\\n",
    "            'registeredVoters' : int(registeredVoters),\\\n",
    "            'votesPolled' : int(votesPolled),\\\n",
    "            'validVotes' : int(validVotes),\\\n",
    "            'rejectedVotes' : int(rejectedVotes),\\\n",
    "            'polledToRegRatio' : float(polledToRegisteredRatio) / 100,\\\n",
    "            'numberOfCandidates' : len(votingResults),\\\n",
    "            'votingResults' : votingResults,\n",
    "            }\n",
    "    \n",
    "    dataframe.append(data)\n",
    "    print('--- ' + seat + ' data addedd succesfully.')\n",
    "    count += 1\n",
    "    \n",
    "    # Sleep for a few seconds before moving on to the next seat\n",
    "    sleep_seconds = randint(2,10)\n",
    "    print('--- Sleeping For : ', sleep_seconds, ' seconds.\\n')\n",
    "    sleep(sleep_seconds)\n",
    "    \n",
    "print('\\n--- ' + str(count) + ' seats processed.')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will also retrieve voter participation rates, which is also bifurcated by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "participationResult = []\n",
    "\n",
    "r = requests.get('https://www.ecp.gov.pk/frmstats.aspx',headers=header)\n",
    "soup = BeautifulSoup(r.text, \"lxml\")\n",
    "rows = soup.find_all('tr')\n",
    "\n",
    "for i in rows[1:]: # We don't look at the first row since these are just headers.\n",
    "    row = i.find_all('td')\n",
    "    participationResult.append(\n",
    "        {\n",
    "        'seat' : row[0].getText(),\n",
    "        'femaleTurnout' : float(row[1].getText().replace(' %','')) / 100,\n",
    "        'maleTurnout' : float(row[2].getText().replace(' %','')) / 100,\n",
    "        'totalTurnout' : float(row[3].getText().replace(' %','')) / 100,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "dfParticipation = pd.DataFrame(participationResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store as a Pandas dataframe and store in a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe)\n",
    "df = df.merge(dfParticipation,on='seat')\n",
    "df.to_csv('Election_2018_NA_Results_Raw.csv',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The voting results is a list of dictionaries contained in the dataframe above. For easier access, we will convert this to it's own dataframe and store it in a CSV as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsOnlyDataframe = []\n",
    "for i in dataframe:\n",
    "    resultsDict = i['votingResults']\n",
    "    for j in resultsDict:\n",
    "        j['seat'] = i['seat']\n",
    "        j['seatName'] = i['seatName']\n",
    "        resultsOnlyDataframe.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResultsOnly = pd.DataFrame(resultsOnlyDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResultsOnly.to_csv('Election_2018_NA_Results_VotingOnly.csv',encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winners & Runner Ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding margins from winners.\n",
    "\n",
    "seat = ''\n",
    "for row in dfResultsOnly.iterrows():\n",
    "    if row[1]['seat'] == seat:\n",
    "        margin = winnerVotes - row[1]['candidateVotes']\n",
    "    else:\n",
    "        winnerVotes = dfResultsOnly[dfResultsOnly['seat'] == row[1]['seat']][['candidateVotes']].max().values[0]\n",
    "        margin = winnerVotes - row[1]['candidateVotes']\n",
    "        seat = row[1]['seat']\n",
    "\n",
    "    dfResultsOnly.loc[row[0],'marginFromWinner'] = int(margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Winners DF & CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winnerIdx = dfResultsOnly.groupby(by='seat',)[['candidateVotes']].idxmax()\n",
    "winnerResults = dfResultsOnly.iloc[winnerIdx['candidateVotes'].values]\n",
    "winnerResults = winnerResults.drop('marginFromWinner',axis=1)\n",
    "winnerResults = winnerResults.merge(runnerupResults[['seat','marginFromWinner']],on='seat').rename(columns={'marginFromWinner' : 'winMargin'})\n",
    "winnerResults.to_csv('winnerResults.csv',index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runner Ups DF & CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runnerUpIdx = dfResultsOnly.groupby(by='seat')['candidateVotes'].nlargest(2).reset_index().groupby('seat').last()['level_1'].values\n",
    "runnerupResults = dfResultsOnly.iloc[runnerUpIdx]\n",
    "runnerupResults.to_csv('runnerupResults.csv',index=False,encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
